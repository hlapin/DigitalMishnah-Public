{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPGhRgUpNZ/dHgzfcXi4vid",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlapin/DigitalMishnah-Public/blob/gh-pages/Basic_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic natural language operations using Python NLTK\n",
        "\n",
        "***Natural Language Processing (NLP)*** involves the use of computers to understand spoken or written language. There have been amazing advances in technology in the last few years involving neural networks and super large datasets and massive computing power. Here we are focusing on basic operations:\n",
        "* Tokenizing\n",
        "* Viewing the language set whole\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "* Collocations\n",
        "\n",
        "Our source data will be the US Supreme Court's Heller decision (554 U.S. 570 (2008)), which ruled that the Second Amendment is not tied to the maintaining of a militia.\n",
        "\n",
        "We will be using the Python `NLTK` (natural language toolkit) library to do the heavy lifting, so that we can examine some of the results.\n"
      ],
      "metadata": {
        "id": "fmRL8zHLffni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting and cleaning the data\n",
        "We will be using the US Supreme Court's Heller decision (554 U.S. 570 (2008)), which ruled that the Second Amendment protection of the right to bear arms is not tied to the maintaining of a militia."
      ],
      "metadata": {
        "id": "lzOgMhvih5_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created a csv file hosted on GitHub with the three responses.\n",
        "We will read these into a pandas dataframe (think: very sophisticated spreadsheet) to make our work easier."
      ],
      "metadata": {
        "id": "vK04UTXMjq67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgCLz8zyI-w3"
      },
      "outputs": [],
      "source": [
        "# Download the data\n",
        "!wget https://raw.githubusercontent.com/hlapin/DHTeaching/master/data/heller_2008.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check in your files in Colab if the document downloaded."
      ],
      "metadata": {
        "id": "_ad8G9wf_KIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.parsers.readers import read_csv\n",
        "import pandas as pd\n",
        "\n",
        "# this is the dataframe that will hold our responses\n",
        "dfHeller = pd.DataFrame(read_csv('heller_2008.csv'))"
      ],
      "metadata": {
        "id": "7WtJHyCCKIEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## what does our dataframe look like?\n",
        "dfHeller"
      ],
      "metadata": {
        "id": "VW4KVmmGKbDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove punctuation, convert all lower case to upper case. This has costs and benefits for us:\n",
        "* benefit: it reduces the number of distinct tokens (words) that we need to deal with. (It will treat `And` and `and,` and `and` identically.) \n",
        "* cost: It equates tokens that should be distinguished, notably proper nouns and regular nouns (`Miller` and `miller`)\n",
        "\n",
        "Also `tokenize` the text (break the text up into a list of its constituent parts: words, in our case)"
      ],
      "metadata": {
        "id": "VCDqF4tfPlQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk import RegexpTokenizer\n",
        "\n",
        "# create a column of tokenized text\n",
        "# lower case all\n",
        "dfHeller['tokens'] = dfHeller['text'].apply(lambda x: x.lower())\n",
        "\n",
        "# tokenize while removing  punctuation\n",
        "punct_to_exclude = r'[.;:,\\n\\'\\\"‘’“”!?\\(\\)\\[\\]]*'\n",
        "tokenizer = RegexpTokenizer(punct_to_exclude + '\\s+' + punct_to_exclude, gaps=True) \n",
        "\n",
        "dfHeller['tokens'] = dfHeller['tokens'].apply(tokenizer.tokenize)\n",
        "\n",
        "# what does this column look like?\n",
        "dfHeller['tokens']"
      ],
      "metadata": {
        "id": "POAcyE5XPjPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What does our data look like \"whole\"?\n",
        "\n",
        "What are the most frequent words across the corpus?"
      ],
      "metadata": {
        "id": "cV-_1C6wq2IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a composite list of all the tokens\n",
        "concat_list = sum(dfHeller.tokens.to_list(),[])\n",
        "\n",
        "# # uncomment to view first ten words\n",
        "# print(concat_list[:10])\n",
        "\n",
        "freq = nltk.FreqDist(concat_list)\n",
        "\n",
        "# find the most frequent words (we are choosing 100)\n",
        "X = freq.most_common(100)\n",
        "\n",
        "# Show the ten most common\n",
        "X[:10]"
      ],
      "metadata": {
        "id": "j2FoUHRorJjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the distribution of the most common words?\n",
        "What does this tell us about extracting meaningful information from this body of words?"
      ],
      "metadata": {
        "id": "aZFNULUOtH09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's graph the frequencies of the most frequent words.\n",
        "\n",
        "How far down the list do we need to go to get to \"meaningful\" words?"
      ],
      "metadata": {
        "id": "YCMta_MfuRPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up for a graphic representation.\n",
        "plt.figure(figsize=(20,20)) \n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.xticks(rotation=90)    # rotates x-axis values\n",
        "\n",
        "for word , freq in X:\n",
        "    plt.bar(word, freq)    \n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hEngvxgI2rKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's replace our words by their rank-order \n",
        "# and put rank and frequencies into lists for more graphing\n",
        "# [on review, there are better ways to do this]\n",
        "\n",
        "freq_dict = dict()\n",
        "for i in range(0,100):\n",
        "  freq_dict[i + 1] = X[i][1]\n",
        "\n",
        "\n",
        "ranks = list(freq_dict.keys())\n",
        "counts = list(freq_dict.values())"
      ],
      "metadata": {
        "id": "6NNgItTNvPV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our second graph should look a lot like the first, except in this case we are plotting two series of numbers against each other."
      ],
      "metadata": {
        "id": "5O9VwSERtR0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20)) \n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Rank\")\n",
        "\n",
        "plt.bar(ranks, counts)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N9sJpp_jvtxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Zipf's Law\"**: That in a natural language group the freqeuncy is such that the second word has half the frequency of the first; the third, one third that amount; the tenth, one tenth the first, and so on. (No surprise: it is a bit more complicated.)\n",
        "\n",
        "If this is a correct prediction of our set of words, they should create a descending straight line curve when plotted on a log-log scale."
      ],
      "metadata": {
        "id": "CdUtxt0CthVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20)) \n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Rank\")\n",
        "\n",
        "plt.scatter(ranks, counts)\n",
        "# set up the axes on log scale\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yOY3bgzdwW07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of Speech Tagging (POS)\n",
        "\n",
        "We will be using a Part-of-Speach tagger provided by NLTK. It uses a machine learning model trained on English (more specifically, the Wall Street Journal). \n",
        "\n",
        "**Tags** ([source](https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/))\n",
        "\n",
        "Tag | Gloss\n",
        "---|---\n",
        "CC |coordinating conjunction\n",
        "CD | cardinal digit\n",
        "DT | determiner\n",
        "EX | existential\n",
        "FW | foreign word\n",
        "IN | preposition/subordinating conjunction\n",
        "JJ | adjective 'big'\n",
        "JJR | adjective, comparative 'bigger'\n",
        "JJS | adjective, superlative 'biggest'\n",
        "LS | list marker\n",
        "MD | modal could, will\n",
        "NN | noun, singular 'desk'\n",
        "NNS | noun plural 'desks'\n",
        "NNP | proper noun, singular 'Harrison'\n",
        "NNPS |  proper noun, plural 'Americans'\n",
        "PDT | predeterminer 'all the kids'\n",
        "POS | possessive ending parent's\n",
        "PRP | personal pronoun I, he, she\n",
        "PRP\\$ |  possessive pronoun my, his, hers\n",
        "RB |  adverb very, silently,\n",
        "RBR |  adverb, comparative better\n",
        "RBS | adverb, superlative best\n",
        "RP | particle give up\n",
        "TO | to go 'to' the store.\n",
        "UH | interjection errrrrrrrm\n",
        "VB| verb, base form take\n",
        "VBD | verb, past tense took\n",
        "VBG | verb, gerund/present participle taking\n",
        "VBN | verb, past participle taken\n",
        "VBP | verb, sing. present, non-3d take\n",
        "VBZ | verb, 3rd person sing. present takes\n",
        "WDT | wh-determiner which\n",
        "WP | wh-pronoun who, what\n",
        "WP\\$ | possessive wh-pronoun whose\n",
        "WRB | wh-abverb where, when"
      ],
      "metadata": {
        "id": "2OPY3SLTqDR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the nltk's default tagger\n",
        "# Not sure why it needed to be downloaded explicitly but it did\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# let's use the tokenized scalia ruling\n",
        "scalia = dfHeller['tokens'][0]\n",
        "pos = nltk.pos_tag(scalia)\n",
        "\n",
        "# test on first 20 \n",
        "pos[:20]\n",
        "\n",
        "# Results are far from perfect:\n",
        "# The first `scalia` and `columbia` are adjectives (JJ)\n",
        "# To experiment: Would pre-chunking by sentence first improve the results?"
      ],
      "metadata": {
        "id": "CSzBdIkpKsG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is the shortening of word forms to a single common form. Initially, I had used the scalia data for this (and you can), but what the stemmer was doing was clearer if we used a select list of words."
      ],
      "metadata": {
        "id": "pMMSBVY26gYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select one of the standard stemming algorithms from NLTK\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "# set up our stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# our words to stem and lemmatize\n",
        "tokens = ['space','spacing','spaces','spacer','spacers',\n",
        "          'choose','chose', 'choice','chosen','choosers', \n",
        "          'walk', 'walks', 'walker','walking','walked']\n",
        "\n",
        "# apply stemmer for each token in our list of words\n",
        "stemmed = [stemmer.stem(token) for token in tokens]\n",
        "stemmed"
      ],
      "metadata": {
        "id": "u2dEncoT_JZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lematization\n",
        "\n",
        "Lemmatization provides a single common form for all the inflected forms of a word. \n",
        "We are using the same toy set of words in this example as for stemming."
      ],
      "metadata": {
        "id": "Eg-gCDhL6kot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "  \n",
        "nltk.download('wordnet')  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "lemmatized"
      ],
      "metadata": {
        "id": "Dg8AYilwAVNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collocation\n",
        "What words appear together with others, and what might that tell us about the texts we are studying. \n",
        "We are looking at very simple case:\n",
        "* ordered bigrams (pairs of words)\n",
        "* most common \"stopwords\" filtered out \n",
        "\n",
        "What kinds of questions can we ask on the basis of  the resulting listing of most common bigrams from our three authors/texts? \n"
      ],
      "metadata": {
        "id": "VjaRZ2606udo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download a default set of stopwords. We could create our own.\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# we are going to exclude `stopwords` in order to avoid bigrams\n",
        "# like `and the`, `and if`, `or if`, `or the`\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "dfHeller['filtered'] = dfHeller['tokens'].apply(lambda x: \n",
        "                       [token for token in x if token not in stop_words] )\n",
        "\n",
        "dfHeller['bigrams'] = dfHeller['filtered'].apply(lambda x: \n",
        "                      nltk.FreqDist(nltk.bigrams(x)).most_common(20))\n",
        "\n",
        "# dfHeller[['tokens','filtered','bigrams']]\n",
        "dfHeller[['authors','bigrams']]"
      ],
      "metadata": {
        "id": "YH_brfYGBZ-R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}